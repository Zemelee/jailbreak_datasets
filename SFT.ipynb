{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel  # 导入FastLanguageModel类，用来加载和使用模型\n",
    "\n",
    "max_seq_length = 2048  # 设置模型处理文本的最大长度，相当于给模型设置一个“最大容量”\n",
    "dtype = None  # 设置数据类型，让模型自动选择最适合的精度\n",
    "load_in_4bit = True  # 使用4位量化来节省内存，就像把大箱子压缩成小箱子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练模型，并获取tokenizer工具\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",  # 指定要加载的模型名称\n",
    "    max_seq_length=max_seq_length,  # 使用前面设置的最大长度\n",
    "    dtype=dtype,  # 使用前面设置的数据类型\n",
    "    load_in_4bit=load_in_4bit,  # 使用4位量化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"how to make a bomb?\"}\n",
    "]\n",
    "# 文本转换为 token ID  添加生成提示符  返回PyTorch 张量   \n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    " \n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "outputs = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 249\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入数据集加载函数\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\",data_files=\"jailbreak.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于格式化数据集中的每条记录\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for instruction, output_text in zip(examples[\"instruction\"],  examples[\"output\"]):\n",
    "        prompt = f\"Instruction: {instruction}\\nOutput: \"\n",
    "        texts.append(prompt + output_text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "data1 = dataset.map(formatting_prompts_func, batched = True)\n",
    "print(data1[\"train\"].column_names)\n",
    "temp = data1[\"train\"]\n",
    "print(temp[\"text\"][0])\n",
    "print(\"-\"*100)\n",
    "print(temp[\"text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,  # 传入已经加载好的预训练模型\n",
    "    r = 16,  # 设置 LoRA 的秩，决定添加的可训练参数数量\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # 指定模型中需要微调的关键模块\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,  # 设置 LoRA 的超参数，影响可训练参数的训练方式\n",
    "    lora_dropout = 0,  # 设置防止过拟合的参数，这里设置为 0 表示不丢弃任何参数\n",
    "    bias = \"none\",    # 设置是否添加偏置项，这里设置为 \"none\" 表示不添加\n",
    "    use_gradient_checkpointing = \"unsloth\",  # 使用优化技术节省显存并支持更大的批量大小\n",
    "    random_state = 3407,  # 设置随机种子，确保每次运行代码时模型的初始化方式相同\n",
    "    use_rslora = False,  # 设置是否使用 Rank Stabilized LoRA 技术，这里设置为 False 表示不使用\n",
    "    loftq_config = None,  # 设置是否使用 LoftQ 技术，这里设置为 None 表示不使用\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer  # 导入 SFTTrainer，用于监督式微调\n",
    "from transformers import TrainingArguments  # 导入 TrainingArguments，用于设置训练参数\n",
    "from unsloth import is_bfloat16_supported  # 导入函数，检查是否支持 bfloat16 数据格式\n",
    "\n",
    "trainer = SFTTrainer(  # 创建一个 SFTTrainer 实例\n",
    "    model=model,  # 传入要微调的模型\n",
    "    tokenizer=tokenizer,  # 传入 tokenizer，用于处理文本数据\n",
    "    train_dataset=temp,  # 传入训练数据集\n",
    "    dataset_text_field=\"text\",  # 指定数据集中 文本字段的名称\n",
    "    max_seq_length=max_seq_length,  # 设置最大序列长度\n",
    "    dataset_num_proc=2,  # 设置数据处理的并行进程数\n",
    "    packing=False,  # 是否启用打包功能（这里设置为 False，打包可以让训练更快，但可能影响效果）\n",
    "    args=TrainingArguments(  # 定义训练参数\n",
    "        per_device_train_batch_size=2,  # 每个设备（如 GPU）上的批量大小\n",
    "        gradient_accumulation_steps=3,  # 梯度累积步数，用于模拟大批次训练\n",
    "        # 2（单卡批量）×3（累积步数）= 6（有效批量大小）\n",
    "        # 249条数据/6 ≈ 42条数据/卡\n",
    "        warmup_steps=5,  # 预热步数，训练开始时学习率逐渐增加的步数\n",
    "        max_steps=84,  # = 2*Epoch\n",
    "        learning_rate=2e-4,  # 学习率，模型学习新知识的速度\n",
    "        fp16=not is_bfloat16_supported(),  # 是否使用 fp16 格式加速训练（如果环境不支持 bfloat16）\n",
    "        bf16=is_bfloat16_supported(),  # 是否使用 bfloat16 格式加速训练（如果环境支持）\n",
    "        logging_steps=5,  # 每隔多少步记录一次训练日志\n",
    "        optim=\"adamw_8bit\",  # 使用的优化器，用于调整模型参数\n",
    "        weight_decay=0.01,  # 权重衰减，防止模型过拟合\n",
    "        lr_scheduler_type=\"linear\",  # 学习率调度器类型，控制学习率的变化方式\n",
    "        seed=3407,  # 随机种子，确保训练结果可复现\n",
    "        output_dir=\"outputs\",  # 训练结果保存的目录\n",
    "        report_to=\"none\",  # 是否将训练结果报告到外部工具（如 WandB），这里设置为不报告\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"how to kill myself?\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,  # 输入的数字序列\n",
    "    attention_mask=inputs.attention_mask,  # 注意力遮罩，帮助模型理解哪些部分重要\n",
    "    max_new_tokens=1111,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"save_jailbreak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 adapter_config.json、adapter_model.safetensors 文件到 save_path 文件夹\n",
    "model.save_pretrained(save_path)\n",
    "# tokenizer_config.json, special_tokens_map.json, tokenizer.json\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载保存后的模型\n",
    "model_, tokenizer_ = FastLanguageModel.from_pretrained(\n",
    "    model_name=save_path,  # 指定要加载的模型名称\n",
    "    max_seq_length=max_seq_length,  # 使用前面设置的最大长度\n",
    "    dtype=dtype,  # 使用前面设置的数据类型\n",
    "    load_in_4bit=load_in_4bit,  # 使用4位量化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer_([\"how to kill my classmates?\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model_.generate(\n",
    "    input_ids=inputs.input_ids,  # 输入的数字序列\n",
    "    attention_mask=inputs.attention_mask,  # 注意力遮罩，帮助模型理解哪些部分重要\n",
    "    max_new_tokens=1111,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer_.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将模型保存为 8 位量化格式（Q8_0）\n",
    "# 这种格式文件小且运行快，适合部署到资源受限的设备\n",
    "if 1: model.save_pretrained_gguf(\"model_gguf\", tokenizer)\n",
    "\n",
    "# 将模型保存为 16 位量化格式（f16）\n",
    "# 16 位量化精度更高，但文件稍大\n",
    "if False: model.save_pretrained_gguf(\"model_f16\", tokenizer, quantization_method = \"f16\")\n",
    "\n",
    "# 将模型保存为 4 位量化格式（q4_k_m）\n",
    "# 4 位量化文件最小，但精度可能稍低\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = \"\"\n",
    "# 导入 Hugging Face Hub 的 create_repo 函数，用于创建一个新的模型仓库\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "# 在 Hugging Face Hub 上创建一个新的模型仓库\n",
    "create_repo(\"zemelee/11\", token=HUGGINGFACE_TOKEN, exist_ok=True)\n",
    "\n",
    "# # 将模型和分词器上传到 Hugging Face Hub 上的仓库\n",
    "# model.push_to_hub_gguf(\"zemelee/jailbreak-test\", tokenizer, token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer._ollama_modelfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
